#+TITLE: Arquitectura y Programación de Altas Prestaciones

* Datos de la asignatura
- Profesora: Maribel García Arenas ([[mgarenas@ugr.es]])
  Despacho 32 (2ª planta)

Lenguajes: C, C++, Java, Fortran

* Tareas

** DONE Elegir problema para prácticas 3 y 6

** DONE Paralelizar programa cpi-seq para MPI

** DONE GRUPO Buscar cómo son las máquinas...
...de Fujitsu para HPC según las clasificaciones, ver el sistema de memoria

- 2 transparencias, no más
- -> día 7/3

** DONE GRUPO: Buscar dos benchmark...
para un sistema HPC, explicar en qué consiste el benchmark y su salida.
-> martes 21
Buscar dos bechmark para un sistema HPC, explicar
en qué consiste el bechmark y su salida.

** TODO GRUPO: Problema - ordenación de vectores mediante mergesort

Incluir los códigos en la presentación y concretar qué patrones, estructuras de programa y de datos se están usando.

Mergesort es ejemplo de 'shared data'

=> después de semana santa

** Instrucciones para trabajos -- grupo 3

1 persona busca información
2 personas revisan y seleccionan
1 persona monta la presentación
1 persona la cuenta

Las tareas rotan en cada trabajo
* Teoría
** 1. Arquitecturas MIMD

*** HPC

Un punto clave de High Performance Computing es el aprovechamiento de toda la infraestructura.

Ejemplos de uso:
- Ingeniería asistida por ordenador
- Modelado molecular
- Análisis de genoma
- Modelado numérico

*** Clasificación de arquitecturas MIMD

Enfoque clásico: taxonomía de Flynn (SISD, SIMD, ~MISD~, MIMD)

**** Otros criterios de clasificación
- estructurales
- de control

***** Distribución física de memoria
- Centralizadas
- distribuidas
***** Espacio de direcciones
- único
- múltiple
***** Tiempo de acceso a memoria
Decidido por el tiempo que tarda un procesador en acceder a cualquier dirección de memoria.
- UMA
- NUMA
***** Red de interconexión
- Indirecta (en los nodos de la red no hay cpus)
- Directa (hay cpus en los nodos de la red)
***** Multicomputadores vs multiprocesadores
Tendencias actuales en HPC, tener nodos que son multiprocesadores conectados entre sí formando un multicomputador

**** Multicomputadores
- tiempos de acceso a memoria inferior (menor latencia si es memoria local)
- más escalables o ampliables
- sincronización por mensajes
- programación más compleja
- paso de mensajes
  - síncrono
  - asíncrono

**** Multiprocesadores
- tiempos de acceso a memoria mayor (mayor latencia según el número de conflictos)
- sincronización por hardware => programación más sencilla
- zonas de memoria compartida

Pueden ser
- UMA
- NUMA
  - CC-NUMA
  - COMA

CC-NUMA y COMA se diferencian en el protocolo de coherencia usado.

*** Evaluación de prestaciones

**** Factores que limitan la escalabilidad

***** Limitaciones del algoritmo

Notaremos =p= a la parte paralela y diremos que la ejecución del algoritmo tarda una unidad de tiempo: =s= + =p= = 1.

- El reparto de las tareas cuando hay diferentes unidades de ejecución no es equitativo, a causa de diferencias de tamaño y dependencias entre tareas
- Todos los algoritmos tienen una parte secuencial =s=
- En concreto, el reparto y la recogida de tareas serán secuenciales

***** Otras limitaciones
- Startup overheads (lanzamiento de procesos)
- Cuellos de botella: Uso de recursos compartidos.
- Las comunicaciones siempre serán en serie (sólo hay una tarjeta de red! y si hay más de una, los accesos a memoria serán serializados)

**** Prestaciones

***** Mediciones de tiempos

- Definir lo que se va a considerar trabajo a paralelizar
- No se deben de tener en cuenta a la hora de medir tiempos: entrada/salida (lectura y escritura de ficheros)
- Incluir el wall time --tiempo que está el proceso en CPU

***** Escalabilidad paralela

- ¿Cómo va de rápido más con N trabajadores?
- ¿Cuánto trabajo más podemos hacer con N trabajadores?
- ¿Cómo impacta la comunicación en la ejecución paralela?
- ¿Cuántos recursos están siendo utilizados productivamente?

***** Medidas de escalabilidad

*Escalabilidad fuerte*: Supondremos que la parte secuencial =s= es fija, y la parte paralela =p= se reparte en el número de trabajos paralelos. La cantidad de trabajo realizado es siempre la misma.

*Escalabilidad débil*: Se realiza mayor cantidad de trabajo en el mismo tiempo.

***** Leyes simples de escalabilidad

****** Ley de Amdahl (Productividad: Trabajo / Tiempo)

Asume que la cantidad de trabajo realizada es siempre la misma. De esta se deduce la Ley de Amdahl.

****** Ley de Gustafson

Asume que la cantidad de trabajo va aumentando conforme se aumenta el número de procesos. Demuestra que, de esta forma, es posible conseguir ganancias superlineales, a diferencia de Amdahl que afirmaba que no era posible.

Hay que usar esta ley cuando la cantidad de trabajo es variable (e.g. si el trabajo aumenta o si el algoritmo tiene componentes aleatorias).

****** Situación intermedia a Amdahl y Gustafson

***** Eficiencia paralela

$\varepsilon = \frac{perf_N}{N\times perf_1} = \frac{speedup}{N}$

**** Mejora de prestaciones básicas

- Salir de bucles =break= y saltar iteraciones =continue= cuando ahorre instrucciones

- Evitar operaciones costosas (construir tablas con datos calculados)

- Reducir cantidad de memoria (ajustar los tipos para que ocupen lo necesario)

- Evitar saltos (ifs) cuando sea posible

- Adaptarse al conjunto de instrucciones

- Optimizaciones del compilador: funciones /inline/, alineamiento de variables, optimizar con registros

**** Balanceo de carga

**** Jitter

Si hay muchos procesos el SO tiene alta probabilidad de interrumpir cosas.

** 2. Modelos de Programación paralela adaptados a la arquitectura

*** Encontrar concurrencia

**** Descomposición de tareas

Los algoritmos deben tener
- Flexibilidad: adaptarse a las modificaciones del problema
- Eficiencia: "si ejecutamos en n procesos, debe haber n tareas"
- Simplicidad

Hay compiladores que tratan de extraer paralelismo de las tareas. Funcionan mal generalmente.

Normalmente se empieza a repartir unidades de ejecución muy pequeñas.

Las tareas deben permitir ser de tamaño variable, compensar el esfuerzo de crear hebras/procesos para ellas, y fácilmente depurables.

**** Descomposición de datos

Los datos deben ser descomponibles. Para ello es necesario conocimiento sobre el problema. La descomposición es necesaria si estos datos representan la parte de computación intensiva del algoritmo.

En memoria compartida ~> descomposición de datos = descomposición de tareas

**** Ejemplo: difusión del calor en un sólido

Un cuerpo sólido se puede representar en una estructura tridimensional (tensor). 

- Descomposición de datos: partir la matriz por la mitad y dejar que cada proceso calcule la temperatura en el instante siguiente en toda esa matriz.

- Descomposición de tareas: se reparte el cálculo de la temperatura siguiente en cada punto a lo largo de los procesos (un proceso no se encarga siempre del mismo punto necesariamente).

**** Ejemplo: Imágenes médicas PET (positron emission tomography)

Para mejorar la imagen obtenida en bruto, se modela el cuerpo humano y se simulan muchas otras trayectorias de partículas.

- Descomposición de tareas: la simulación de cada trayectoria va a un proceso (cada una necesita los datos del cuerpo completo)

- Descomposición de datos: partir el cuerpo en varios trozos y que cada proceso simule trayectorias sobre esos trozos (comunicaciones cuando una trayectoria pase de un trozo a otro)

**** Ejemplo: Multiplicación de matrices

Puede que convenga trasponer la segunda matriz para acceder "por filas", verificando así el principio de localidad espacial.

**** Análisis de dependencias

***** 1. Agrupar (para que las dependencias sean menores)

Crucial conocimiento sobre el problema en este paso.

***** 2. Ordenar (para cumplir restrucciones en el procesamiento)

Dependencias:
- Temporales
- Simultaneidad
- Independencia

Generar grafo de tareas.

***** 3. Patrones de compartición (¿cómo pasar los datos entre los grupos de tareas?)

Tener claro qué tareas tienen acceso y con qué permisos =>
Identificar estructuras de datos a compartir y si son read-only, locales (e.g. suma acumulativa) o rw.

Estructuras /read-write/:

- operaciones acumulativas (e.g. reducciones)
- multiple read, single write: cada tarea escribe su propio dato de vez en cuando

**** Evaluación


*** Patrones de algoritmos

Conflictos:
- eficiencia vs portabilidad
- eficiencia vs simplicidad

**** Plataforma de ejecución

Idealmente: no tener en cuenta la plataforma => pérdida de eficiencia

4 puntos:
- ¿cuántos procesadores efectivos?
- ¿cómo repartir el trabajo y con qué coste?
- proteger la flexibilidad
- entornos de programación

**** Principio de organización

3 estructuras:
- organización por tareas
- descomposición de datos
- flujo de datos

Posibles situaciones:
- sólo existe un grupo de tareas activas en cada momento
- manejo de grandes volúmenes de datos (evitar los accesos secuenciales)
- grupos de tareas bien definidas que interactúan entre ellas pasando resultados de un conjunto a otro

**** Paralelismo de tareas

**** 

*En CUDA no hay contadores de programa distintos para cada hebra*. Por esto, las hebras deben hacer el mismo trabajo (incluso intentar que no haya if-then-elses puesto que las hebras que no ejecuten la parte =then= se pararán, no pudiendo ejecutar el =else= a la vez).

***** Tareas independientes

Ejemplo: conjunto de Mandelbrot

***** Tareas definidas recursivamente

=> Divide y vencerás

- Las tareas no son exactamente iguales => balanceo de carga

***** Coordinación basada en eventos

*** Estructuras disponibles
**** Estructuras de programas

- SPMD (GPUs)
- Master + workers
- Paralelismo a nivel de bucle (openMP)
- Fork...Join (paralelización dinámica estilo servidor web)


***** SPMD

Pasos:
- Inicializar
- Obtener identificación única
- Repartir datos cuando proceda
- Ejecutar el mismo programa
- Recopilar resultados si procede
- Finalizar

***** Maestro-trabajador

- Dificultad: el balanceo de carga
- Algoritmo robusto: se puede recuperar si un trabajador se cae
- La gestión del maestro es complicada en general

El maestro puede ser también trabajador.

La cola de tareas puede ser un cuello de botella. Para solucionarlo se puede construir un árbol de maestros.

Tolerancia a fallos: mantener dos colas, una de pendientes (enviadas) y otra de terminadas.

***** Paralelismo a nivel de bucle

OpenMP no puede tener ganancias superlineales porque necesariamente hay una parte secuencial y no hay uso de escalabilidad débil.

Pasos:
- Identificar bottlenecks
- Eliminar dependencias de las iteraciones entre sí
- Paralelizar bucles
- Optimizar planificación del bucle con las unidades disponibles


***** Fork ... join

Ejemplo: mergesort con mapeo directo o indirecto


**** Estructuras de datos

- Datos compartidos
- Colas compartidas (colas abstractas thread safe)
- Arrays distribuidos (arrays particionados)

Necesitamos:
- escalabilidad
- claridad de la abstracción
- eficiencia
- mantenimiento
- afinidad al entorno de programación
- equivalencia (si es posible) a la versión secuencial


***** Datos compartidos

- La estructura de datos la modifica al menos una tarea
- El resto leen
- La "propietaria" debe controlar el acceso a la estructura, que ha de minimizarse

Se deben controlar situaciones de carrera <= el resultado debe ser correcto independientemente de la ordenación de las tareas
Puede que el número de tareas simultáneas leyendo la estructura esté limitado (e.g. en bases de datos)

Pasos:
- Usar como último recurso => asegurarse de que es necesario
- Definir un tipo de dato con operaciones básicas
- Decidir o implementar un protocolo de acceso concurrente a la estructura. Ejemplos:
  - One-at-a-time (cerrojos)
  - Tareas que no interfieran
  - Organizar tareas escritoras y otras lectoras
  - Replicar partes de la estructura


***** Array (tensor) distribuido

- Necesita balanceo de carga
- Manejo efectivo de memoria

**** Estructura de programa según patrón y entorno de programación [diapositivas]

Paralelizar bucles en CUDA no siempre tiene sentido porque necesitamos un número de iteraciones muy alto para no desaprovechar cores, pero cuando tenemos muchas iteraciones las estructuras de datos son muy grandes y la copia de datos a la memoria de la GPU cuesta!

*** Algoritmos más comunes

**** Reducción

Las reducciones se realizan sobre un monoide.

Los accesos a memoria shared se intentan realizar a líneas de memoria lejanas de forma que no vayan todas las peticiones al mismo sitio en la memoria y se puedan realizar en paralelo.

**** Scan



**** Convolución (discreta)


** 3. Redes de Interconexión

* Prácticas

** Notas

*** Ejecución de programas / toma de medidas
- Los ordenadores con id~140xxx y 142xxx no comparten subred, luego no usarlos para tomar tiempos
- Al ejecutar programas sobre una sola máquina usar la versión paralela en secuencial (no un programa distinto que realice la misma tarea y tenga menos instrucciones...) ~> afecta al cálculo de la ganancia de velocidad.
- No vamos a evaluar las "otras posibles medidas"
- Ejecutar en atcgrid con =/usr/lib64/openmpi/bin/mpiexec=
- Medir tiempos con =MPI_Wtime()= en MPI o =omp_get_wtime= en OpenMP o =clock_gettime(CLOCK_REALTIME, &cgt1)= de C
*** MPI

En MPI se pueden numerar los procesos de forma bidimensional.

*** CUDA
- CUDA está pensado únicamente para floats, no enteros
- Las hebras se organizan en bloques, y dependiendo del bloque pueden tener identificaciones uni, bi, y tri-dimensionales.
** 4 y 5. Seminario de CUDA

- leer nombres de ficheros desde la línea de comandos
- abrir ficheros
- leer input0->a, input1->b y output->c
- sumar a+b->d
- comparar c, d (usar una tolerancia?)
- imprimir la suma (en la cpu, para que el compilador no se lo cargue!)


cuda: /usr/local/cuda-5.0/bin/nvcc -m64 -I/usr/local/cuda-5.0/include

Las hebras que están en el mismo bloque comparten memoria y se pueden sincronizar, las hebras de bloques distintos no.

identificación de hebras (1 dimensión):
- =blockidx.x= nivel de bloque
- =blockDim.x= número de hebras en bloque
- =threadidx.x= nivel de hebra en bloque
- =blockidx.x * blockDim.x + threadidx.x= nivel de hebra global

Cada bloque va siempre al mismo SM aunque haya más hebras en el bloque que cores en el SM.

Geforce GT 750M: "The GK107 Kepler core offers two shader blocks, called SMX, each with 192 shaders for a total of 384 shader cores that are clocked at the same speed as the processor core." [[http://www.notebookcheck.net/NVIDIA-GeForce-GT-750M.90245.0.html][src]]

Las hebras se agrupan en /warps/ de 32 hebras. Interesa que cuando sobren hebras sean en múltiplos de 32 para no desaprovechar trabajo de hebras. Además, el bloque mínimo debería ser de 32 hebras. Probar múltiples tamaños.
