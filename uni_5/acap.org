#+TITLE: Arquitectura y Programación de Altas Prestaciones

* Datos de la asignatura
- Profesora: Maribel García Arenas (mgarenas@ugr.es)
  Despacho 32 (2ª planta)

Lenguajes: C, C++, Java, Fortran

* Tareas

** DONE Elegir problema para prácticas 3 y 6

** DONE Paralelizar programa cpi-seq para MPI

** DONE GRUPO Buscar cómo son las máquinas...
...de Fujitsu para HPC según las clasificaciones, ver el sistema de memoria

- 2 transparencias, no más
- -> día 7/3

** DONE GRUPO: Buscar dos benchmark...
para un sistema HPC, explicar en qué consiste el benchmark y su salida.
-> martes 21
Buscar dos bechmark para un sistema HPC, explicar
en qué consiste el bechmark y su salida.

** TODO GRUPO: Problema - ordenación de vectores mediante mergesort

Incluir los códigos en la presentación y concretar qué patrones, estructuras de programa y de datos se están usando.

Mergesort es ejemplo de 'shared data'

=> después de semana santa

** Instrucciones para trabajos -- grupo 3

1 persona busca información
2 personas revisan y seleccionan
1 persona monta la presentación
1 persona la cuenta

Las tareas rotan en cada trabajo
* Teoría

** 1. Arquitecturas MIMD

*** HPC

Un punto clave de High Performance Computing es el aprovechamiento de toda la infraestructura.

Ejemplos de uso:
- Ingeniería asistida por ordenador
- Modelado molecular
- Análisis de genoma
- Modelado numérico

*** Clasificación de arquitecturas MIMD

Enfoque clásico: taxonomía de Flynn (SISD, SIMD, ~MISD~, MIMD)

**** Otros criterios de clasificación
- estructurales
- de control

***** Distribución física de memoria
- Centralizadas
- distribuidas
***** Espacio de direcciones
- único
- múltiple
***** Tiempo de acceso a memoria
Decidido por el tiempo que tarda un procesador en acceder a cualquier dirección de memoria.
- UMA
- NUMA
***** Red de interconexión
- Indirecta (en los nodos de la red no hay cpus)
- Directa (hay cpus en los nodos de la red)
***** Multicomputadores vs multiprocesadores
Tendencias actuales en HPC, tener nodos que son multiprocesadores conectados entre sí formando un multicomputador

**** Multicomputadores
- tiempos de acceso a memoria inferior (menor latencia si es memoria local)
- más escalables o ampliables
- sincronización por mensajes
- programación más compleja
- paso de mensajes
  - síncrono
  - asíncrono

**** Multiprocesadores
- tiempos de acceso a memoria mayor (mayor latencia según el número de conflictos)
- sincronización por hardware => programación más sencilla
- zonas de memoria compartida

Pueden ser
- UMA
- NUMA
  - CC-NUMA
  - COMA

CC-NUMA y COMA se diferencian en el protocolo de coherencia usado.

*** Evaluación de prestaciones

**** Factores que limitan la escalabilidad

***** Limitaciones del algoritmo

Notaremos =p= a la parte paralela y diremos que la ejecución del algoritmo tarda una unidad de tiempo: =s= + =p= = 1.

- El reparto de las tareas cuando hay diferentes unidades de ejecución no es equitativo, a causa de diferencias de tamaño y dependencias entre tareas
- Todos los algoritmos tienen una parte secuencial =s=
- En concreto, el reparto y la recogida de tareas serán secuenciales

***** Otras limitaciones
- Startup overheads (lanzamiento de procesos)
- Cuellos de botella: Uso de recursos compartidos.
- Las comunicaciones siempre serán en serie (sólo hay una tarjeta de red! y si hay más de una, los accesos a memoria serán serializados)

**** Prestaciones

***** Mediciones de tiempos

- Definir lo que se va a considerar trabajo a paralelizar
- No se deben de tener en cuenta a la hora de medir tiempos: entrada/salida (lectura y escritura de ficheros)
- Incluir el wall time --tiempo que está el proceso en CPU

***** Escalabilidad paralela

- ¿Cómo va de rápido más con N trabajadores?
- ¿Cuánto trabajo más podemos hacer con N trabajadores?
- ¿Cómo impacta la comunicación en la ejecución paralela?
- ¿Cuántos recursos están siendo utilizados productivamente?

***** Medidas de escalabilidad

*Escalabilidad fuerte*: Supondremos que la parte secuencial =s= es fija, y la parte paralela =p= se reparte en el número de trabajos paralelos. La cantidad de trabajo realizado es siempre la misma.

*Escalabilidad débil*: Se realiza mayor cantidad de trabajo en el mismo tiempo.

***** Leyes simples de escalabilidad

****** Ley de Amdahl (Productividad: Trabajo / Tiempo)

Asume que la cantidad de trabajo realizada es siempre la misma. De esta se deduce la Ley de Amdahl.

****** Ley de Gustafson

Asume que la cantidad de trabajo va aumentando conforme se aumenta el número de procesos. Demuestra que, de esta forma, es posible conseguir ganancias superlineales, a diferencia de Amdahl que afirmaba que no era posible.

Hay que usar esta ley cuando la cantidad de trabajo es variable (e.g. si el trabajo aumenta o si el algoritmo tiene componentes aleatorias).

****** Situación intermedia a Amdahl y Gustafson

***** Eficiencia paralela

$\varepsilon = \frac{perf_N}{N\times perf_1} = \frac{speedup}{N}$

**** Mejora de prestaciones básicas

- Salir de bucles =break= y saltar iteraciones =continue= cuando ahorre instrucciones

- Evitar operaciones costosas (construir tablas con datos calculados)

- Reducir cantidad de memoria (ajustar los tipos para que ocupen lo necesario)

- Evitar saltos (ifs) cuando sea posible

- Adaptarse al conjunto de instrucciones

- Optimizaciones del compilador: funciones /inline/, alineamiento de variables, optimizar con registros

**** Balanceo de carga

**** Jitter

Si hay muchos procesos el SO tiene alta probabilidad de interrumpir cosas.

** 2. Modelos de Programación paralela adaptados a la arquitectura

*** Encontrar concurrencia

**** Descomposición de tareas

Los algoritmos deben tener
- Flexibilidad: adaptarse a las modificaciones del problema
- Eficiencia: "si ejecutamos en n procesos, debe haber n tareas"
- Simplicidad

Hay compiladores que tratan de extraer paralelismo de las tareas. Funcionan mal generalmente.

Normalmente se empieza a repartir unidades de ejecución muy pequeñas.

Las tareas deben permitir ser de tamaño variable, compensar el esfuerzo de crear hebras/procesos para ellas, y fácilmente depurables.

**** Descomposición de datos

Los datos deben ser descomponibles. Para ello es necesario conocimiento sobre el problema. La descomposición es necesaria si estos datos representan la parte de computación intensiva del algoritmo.

En memoria compartida ~> descomposición de datos = descomposición de tareas

**** Ejemplo: difusión del calor en un sólido

Un cuerpo sólido se puede representar en una estructura tridimensional (tensor). 

- Descomposición de datos: partir la matriz por la mitad y dejar que cada proceso calcule la temperatura en el instante siguiente en toda esa matriz.

- Descomposición de tareas: se reparte el cálculo de la temperatura siguiente en cada punto a lo largo de los procesos (un proceso no se encarga siempre del mismo punto necesariamente).

**** Ejemplo: Imágenes médicas PET (positron emission tomography)

Para mejorar la imagen obtenida en bruto, se modela el cuerpo humano y se simulan muchas otras trayectorias de partículas.

- Descomposición de tareas: la simulación de cada trayectoria va a un proceso (cada una necesita los datos del cuerpo completo)

- Descomposición de datos: partir el cuerpo en varios trozos y que cada proceso simule trayectorias sobre esos trozos (comunicaciones cuando una trayectoria pase de un trozo a otro)

**** Ejemplo: Multiplicación de matrices

Puede que convenga trasponer la segunda matriz para acceder "por filas", verificando así el principio de localidad espacial.

**** Análisis de dependencias

***** 1. Agrupar (para que las dependencias sean menores)

Crucial conocimiento sobre el problema en este paso.

***** 2. Ordenar (para cumplir restrucciones en el procesamiento)

Dependencias:
- Temporales
- Simultaneidad
- Independencia

Generar grafo de tareas.

***** 3. Patrones de compartición (¿cómo pasar los datos entre los grupos de tareas?)

Tener claro qué tareas tienen acceso y con qué permisos =>
Identificar estructuras de datos a compartir y si son read-only, locales (e.g. suma acumulativa) o rw.

Estructuras /read-write/:

- operaciones acumulativas (e.g. reducciones)
- multiple read, single write: cada tarea escribe su propio dato de vez en cuando

**** Evaluación


*** Patrones de algoritmos

Conflictos:
- eficiencia vs portabilidad
- eficiencia vs simplicidad

**** Plataforma de ejecución

Idealmente: no tener en cuenta la plataforma => pérdida de eficiencia

4 puntos:
- ¿cuántos procesadores efectivos?
- ¿cómo repartir el trabajo y con qué coste?
- proteger la flexibilidad
- entornos de programación

**** Principio de organización

3 estructuras:
- organización por tareas
- descomposición de datos
- flujo de datos

Posibles situaciones:
- sólo existe un grupo de tareas activas en cada momento
- manejo de grandes volúmenes de datos (evitar los accesos secuenciales)
- grupos de tareas bien definidas que interactúan entre ellas pasando resultados de un conjunto a otro

**** Paralelismo de tareas

**** 

*En CUDA no hay contadores de programa distintos para cada hebra*. Por esto, las hebras deben hacer el mismo trabajo (incluso intentar que no haya if-then-elses puesto que las hebras que no ejecuten la parte =then= se pararán, no pudiendo ejecutar el =else= a la vez).

***** Tareas independientes

Ejemplo: conjunto de Mandelbrot

***** Tareas definidas recursivamente

=> Divide y vencerás

- Las tareas no son exactamente iguales => balanceo de carga

***** Coordinación basada en eventos

*** Estructuras disponibles
**** Estructuras de programas

- SPMD (GPUs)
- Master + workers
- Paralelismo a nivel de bucle (openMP)
- Fork...Join (paralelización dinámica estilo servidor web)


***** SPMD

Pasos:
- Inicializar
- Obtener identificación única
- Repartir datos cuando proceda
- Ejecutar el mismo programa
- Recopilar resultados si procede
- Finalizar

***** Maestro-trabajador

- Dificultad: el balanceo de carga
- Algoritmo robusto: se puede recuperar si un trabajador se cae
- La gestión del maestro es complicada en general

El maestro puede ser también trabajador.

La cola de tareas puede ser un cuello de botella. Para solucionarlo se puede construir un árbol de maestros.

Tolerancia a fallos: mantener dos colas, una de pendientes (enviadas) y otra de terminadas.

***** Paralelismo a nivel de bucle

OpenMP no puede tener ganancias superlineales porque necesariamente hay una parte secuencial y no hay uso de escalabilidad débil.

Pasos:
- Identificar bottlenecks
- Eliminar dependencias de las iteraciones entre sí
- Paralelizar bucles
- Optimizar planificación del bucle con las unidades disponibles


***** Fork ... join

Ejemplo: mergesort con mapeo directo o indirecto


**** Estructuras de datos

- Datos compartidos
- Colas compartidas (colas abstractas thread safe)
- Arrays distribuidos (arrays particionados)

Necesitamos:
- escalabilidad
- claridad de la abstracción
- eficiencia
- mantenimiento
- afinidad al entorno de programación
- equivalencia (si es posible) a la versión secuencial


***** Datos compartidos

- La estructura de datos la modifica al menos una tarea
- El resto leen
- La "propietaria" debe controlar el acceso a la estructura, que ha de minimizarse

Se deben controlar situaciones de carrera <= el resultado debe ser correcto independientemente de la ordenación de las tareas
Puede que el número de tareas simultáneas leyendo la estructura esté limitado (e.g. en bases de datos)

Pasos:
- Usar como último recurso => asegurarse de que es necesario
- Definir un tipo de dato con operaciones básicas
- Decidir o implementar un protocolo de acceso concurrente a la estructura. Ejemplos:
  - One-at-a-time (cerrojos)
  - Tareas que no interfieran
  - Organizar tareas escritoras y otras lectoras
  - Replicar partes de la estructura


***** Array (tensor) distribuido

- Necesita balanceo de carga
- Manejo efectivo de memoria

**** Estructura de programa según patrón y entorno de programación [diapositivas]

Paralelizar bucles en CUDA no siempre tiene sentido porque necesitamos un número de iteraciones muy alto para no desaprovechar cores, pero cuando tenemos muchas iteraciones las estructuras de datos son muy grandes y la copia de datos a la memoria de la GPU cuesta!

*** Algoritmos más comunes

**** Reducción

Las reducciones se realizan sobre un monoide.

Los accesos a memoria shared se intentan realizar a líneas de memoria lejanas de forma que no vayan todas las peticiones al mismo sitio en la memoria y se puedan realizar en paralelo.

**** Scan



**** Convolución (discreta)


** 3. Redes de Interconexión
*** Bibliografía

Arquitectura de computadores (Cap 7, 8 y 9) ¡Estudiar del libro! Las transparencias no son suficientes.
*** Tipos de redes de interconexión, estructura general del sistema
**** Comunicación en multicomputadores (send/receive)
**** Comunicación en multiprocesadores (read/write)

La circuitería correspondiente al sistema de coherencia forma parte del sistema de comunicación.
**** Interfaz de red

- DMA (copia de la memoria principal a la memoria local del interfaz)
  - lo hace pidiendo control del bus de memoria, o utilizándolo cuando no lo está usando la CPU o mediante un protocolo
- Memoria
- Estado
- Asistente comunicación
- Controlador de enlace + Cola envío
- Controlador de enlace + Cola recepción
  - El controlador se encarga de que en el destino haya espacio para almacenar la próxima unidad de información que llegue
- Circuiteria específica según fabricante
***** Funciones
- Almacenamiento temporal de paquetes de E/S
- Formateo del paquete
- Copia y almacenamiento de datos
- Control de flujo
- Dirección física, encaminamiento y reconfiguración de la red
  - Hay varias formas de encaminamiento:
    - En fuente: el camino está decidido al completo desde el principio (se encapsula en el mensaje)
    - Decisión etapa a etapa: en cada switch/router, según el destino, se calcula por qué salida debe ir el paquete
**** Switch (conmutador)

Se caracteriza por el número de entradas y el de salidas que tiene.

El encaminador debe encaminar y arbitrar (resolver qué paquete sale antes, de dos paquetes que se han encaminado a la misma salida; en algunos casos se desestima uno de los paquetes (en TCP)).
**** Enlace
Generalmente son unidireccionales. Si son bidireccionales, pueden ser:
- full-duplex (hay hilos dedicados para cada dirección)
- half-duplex (todos los hilos se dedican a un solo sentido, y se cambia el sentido cuando conviene)

Se clasifican respecto a su anchura y longitud:
- Anchura
  - anchos (se transmiten datos y control a la vez)
  - estrechos (se reparte en el tiempo los datos y el control)
- Longitud
  - cortos (1 símbolo/unidad de transmisión/"phit")
  - largos (varios símbolos propagándose simultáneamente)
*** Análisis de prestaciones
**** Extremo a extremo
Entre cualquier nodo de origen y cualquier nodo de destino

Hay muchas latencias
**** Globales
Sólo se puede medir de forma experimental.

Cada nodo genera un patrón de comunicación.

Permiten calcular
- Latencia media
- Productividad global/aceptada
- Productividad solicitada/aplicada
- Productividad máxima

En el momento de saturación, muchos interfaces de red acaban vaciando los buffers y descartando todos los paquetes por enviar.

**** Adicionales

*** Diseño de redes: niveles de servicio**** Cómo diseñar redes

***** Topología

***** Algoritmo de encaminamiento (dónde)

***** Estrategia de conmutación (cómo)

***** Mecanismos de control de flujo (cuándo)

**** Unidad mínima de información

***** Física: phit

***** Enlace (nivel conmutacion y encaminamiento): flit

***** Transporte (nivel mensaje, interfaz de red): paquetes

***** Aplicación: Mensaje

**** Topologías y clasificación

***** Clasificaciones

****** Estáticas

De dimensión $n$ y tamaño $K_{n-1}\times\dots K_1\times K_0$.

******* Estrictas

Cada nodo tiene un enlace al menos en cada dimensión

******** Ortogonales

Cada nodo está conectado a los más próximos (avanzando y retrocediendo una unidad) en cada dimensión.

Se dice que el sentido de un enlace de N a M es positivo si N <lex M, y negativo si M <lex M.

La distancia mínima es la de Manhattan.

e.g. mallas, toros

e.g. el toro de dimensión 2 y 2 x 4 nodos es la misma red que un 3-cubo o una malla de dimensión 3 y base 2.

****** Dinámicas
dinámicas: cada nodo de conmutación no tiene por qué estar ocupado por un nodo de computación.

******* Compartido

Bus de comunicación (1 solo canal!)

******* No compartido

******** Barras cruzadas

Barras cruzadas: cada entrada está conectada con cada salida (es fully connected), con caminos independientes (cada entrada puede tener comunicación simultánea con cada salida, a diferencia de la multietapa).

Se usan mucho si el número de entradas es pequeño.

Se forman por unión de conmutadores y enlaces.

******** Multietapa

Intentan proporcionar la misma funcionalidad que las cruzadas pero ahorrando componentes.

********* Bloqueantes

No necesariamente son posibles comunicaciones simultáneas entre dos cualesquiera pares entrada-salida.

********* No bloqueantes

Son posibles comunicaciones simultáneas entre dos cualesquiera pares entrada-salida.

Se consiguen duplicando enlaces y conmutadores.

********* Reconfigurables

No bloqueante, tiene más de un camino entre cada entrada y cada salida.
Es capaz de re-enrutar una comunicación conflictiva a través de otros enlaces y conmutadores.

********* Tipos

********** Omega

********** Mariposa

********** Cubo

Diapo 51: están mal colocados los números de M_1^2 y M_2^2 (?)

*** Técnicas de conmutación

**** Introducción

Cómo vamos a compararlas:
- cuantitativamente: respecto a la latencia de transporte de un paquete de un tamaño concreto
  - Para el ejemplo, consideraremos 1 flit = 1 phit = w bits
  - La cabecera tendrá 1 flit
  - Datos de L = 3w bits
  - D = número de parejas conmutador enlace de la fuente al interfaz de red del destino
    - Aplicaremos D = dist(origen, destino) + 1
  - Suponemos ausencia de conflictos
- cualitativamente: respecto al ancho de banda global

***** Ejemplo

$t_w$ será el tiempo necesario para transportar $w$ bits en una etapa conmutador-enlace. $t_r$ será el tiempo de enrutamiento.

**** Almacenamiento y Reenvío

Características:
- el conmutador espera a tener el paquete completo para aplicar enrutamiento
- un paquete ocupa no más de un canal (enlace) de la red => afecta poco a las prestaciones globales de la red, pero puede afectar a las prestaciones locales.
- los buffers obviamente tienen que tener capacidad mínima de un paquete
- unidad de transferencia = paquete

Asumiendo que el conmutador tiene almacenamiento único (no distinguimos almacenamiento a la entrada y a la salida):
$$t_{AR}=D\left(t_r+ \left(\mathrm{ceil}\left(\frac L W \right) + 1\right)\times t_w\right)$$

Ejemplo:
Con las constantes tomadas para el ejemplo, $t_{AR} = 3(t_r + 4t_w)$

**** Vermiforme

Características:
- En cuanto llega la cabecera se aplica enrutamiento (una sola vez)
- La transferencia se realiza de forma segmentada
- La unidad de transferencia es el mensaje
- Los bufferes deben tener capacidad mínima de 1 flit
- El ancho de banda global se ve afectado! Más cuanto más largos sean los paquetes

$$t_V=t_{\mathrm{cabecera}}+t_{\mathrm{resto}}=D\times (t_r+t_w) + t_w\times\frac L W$$

No es interesante aprenderse esta fórmula, asume que los conmutadores y demás funcionan como en el ejemplo.

**** Virtual Cut-Through

Características:
- Funciona como almacenamiento y reenvío pero no espera al paquete entero
- Enruta en cuanto llega la cabecera
- La transferencia se realiza de forma segmentada, PERO se pueden reagrupar todos los mensajes de un paquete si un conmutador tiene el enlace de salida ocupado (así no se ve tan afectado el ancho de banda como en vermiforme, pero sí un poco más que en AR)

**** Conmutación de circuitos

Características:
- Se envía una sonda que reserva el camino. El destino devuelve una confirmación
- La transferencia usa todo el ancho de banda (se forma un cauce entre origen y destino)
- La unidad de transferencia es el mensaje
- La capacidad mínima

**** Canales virtuales

Permite que varios canales compartan el mismo enlace a nivel de flit (e.g. en el tick de bajada se pasa un mensaje de un paquete y en el de subida uno de otro)

**** Problemas

**Nota**: Los tiempos de enrutamiento se toman por cada flit, mientras que tiempos de transferencia y sobrecarga se toman por phit. Es muy importante cuando 1 flit > 1 phit

***** Propuesto 3
Imaginemos un multiprocesador de memoria distribuida con red de interconexión toro bidimensional,
canales bidireccionales y 1024 nodos terminales (supongamos igual base en las dos dimensiones). Los
conmutadores de la red tienen buffers asociados a las entradas y a las salidas. Esto hace que en las
técnicas de conmutación que aprovechan para la transferencia la segmentación en etapas del camino
por parte de los buffers, se tenga que tener en cuenta, al obtener la latencia de transporte, que hay
etapas que suponen atravesar un conmutador y etapas que implican atravesar el canal entre
conmutadores. Los tiempos que determinan el retardo de comunicación en el sistema de comunicación
son: t sobrecarga = 1$\mu$s (tiempo de sobrecarga en el fuente + tiempo de sobrecarga en el destino); t_r = 2 ns
(tiempo requerido por el algoritmo de encaminamiento en el conmutador); t w = 6 ns (tiempo de
transferencia de 1 phit entre conmutadores); y t s = 5 ns (tiempo de transferencia de 1phit entre un
buffer de entrada y un buffer de salida de un conmutador)

Supongamos un paquete de 100 phits + 2 phits de cabecera.

Calcular la latencia (téngase en cuenta también la sobrecarga) que supone la transferencia de este
paquete desde el nodo 527 al nodo 56 por el camino más corto:

a. Si se utiliza conmutación vermiforme (consideramos que un flit consta de 2 phits).
b. Si se utiliza conmutación almacenamiento y reenvío.
c. Si se utiliza conmutación de circuitos con camino segmentado (consideramos que un flit consta de 2 phits, y que la sonda y el reconocimiento son un flit).

nodo 527 == (16, 15)
nodo 56 == (1, 24)

dist(527, 56) = 24 => D = 25

b.
$$t_r=2\mathrm{ns}/\mathrm{flit} \times 1 \mathrm{flit}/\mathrm{cabecera}=2\mathrm{ns}$ (si la cabecera ocupa más de 1 flit, se tarda más en enrutarla? no tiene sentido eso)

$$t_{\mathrm{transf-AR}}=102\times t_w + 25 \times\left(t_r + 102 (t_s + t_w)\right)=
102\times 6 + 25(2 + 102(5 + 6))\mathrm{ns}=28712\mathrm{ns}=28.712\mu s$$
$$t = t_{sobrecarga} + t_{\mathrm{transf-AR}} = 1\mu s + 28.712 \mu s= 29.712 \mu s$$

a.
$$t_{\mathrm{transf-V}}=t_{cab}+t_{datos}=2\mathrm{phits}\times t_w + 25(t_r + 2(t_s+t_w))+
100\max\{t_s,t_w\}$$
$$t=t_s+t_{\mathrm{transf-V}}=1.212\mu s$$

c.
$$t_{\mathrm{transf-CC}}=t_{\mathrm{cab}}+t_{\mathrm{sonda}}+t_{\mathrm{datos}}=
2t_w + 25(t_r+2(t_w+t_s))+2t_w+25\times 2(t_w+t_s)+(2t_w+25(2(t_w+t_s))+49\times 2t_w)$$
$$t = t_s + t_{\mathrm{transf-CC}}=3.324\mu s$$


*** Control de flujo

**** Físico

***** Síncrono
- Se tienen relojes sincronizados
- Se envía una señal de sincronización (reloj)

Para cada bajada del reloj se manda un phit de datos.

***** Asíncrono
Se comprueba la recepción cada pocos bits

**** Conmutación

- Debe arbitrar los conflictos
- Debe asegurar el espacio en el destino

Líneas cortas (probablemente flit = phit):
- transferencia asíncrona
- transferencia síncrona

Líneas largas (probablemente flit > phit):
- transferencia síncrona: el emisor tiene un contador de crédito, el reciptor va enviando dicho crédito.


Los fabricantes suelen intentar reducir las líneas de control. Por ello, a la línea de crédito se le suelen aportar más funcionalidad (e.g. STOP and GO).

*** Encaminamiento

Tareas:
- determinar qué caminos se pueden seguir para ir de fuente a destino
- seleccionar el camino concreto a seguir de entre los candidatos

Diseño:
- Funcionalidad
- Decisión de encaminamiento
  - Centralizado
  - Fuente
  - Distribuido
  - Multifase
- Implementación
  - Con tabla de consulta
  - Sin tabla de consulta
- Selección del camino
  - Deterministas: seleccionan siempre el mismo camino
  - Inconscientes: seleccionan un camino, teniendo en cuenta el estado local, sin tener en cuenta el tráfico de la red global
  - adaptativos: tienen en cuenta el estado global de la red
- Canales candidatos y caminos alternativos (función de
encaminamiento)

Grafo de dependencias: establece un vértice por cada canal, y hay una arista entre dos vértices si el algoritmo de encaminamiento permite, viniendo de un canal, salir por el otro.

**** Algoritmos de eliminación de interbloqueos

***** Encaminamiento ordenado por dimensión

Es determinista, impide que los paquetes anden en círculos por la red.
Eliminan todos los interbloqueos en mallas e hipercubos. En toros necesitan además canales virtuales.

***** Algoritmo del intervalo
Es distribuido con tabla y determinista. Es ordenado por dimensión.

El algoritmo del intervalo numera en vertical-first pa que al enrutar horizontal-first lleguemos a un intervalo de nodos.

**** Enrutamiento en redes tipo mariposa unidireccionales

Redes $a^n\times b^m$

**** Encaminamiento Up-Down
Ordenado por dimensión. Se pueden hacer cambios de entre un conjunto. No genera interbloqueos si la topología no los tiene.

Lo que se hace es: se coge un canal del conjunto Up siempre que exista un canal que te acerque (si lo buscamos mínimo) al destino y esté disponible. Se coge únicamente del conjunto Up hasta que deja de haber y luego exclusivamente de Down.

Se utilizan mucho para comunicaciones broadcast.


* Prácticas

** Notas

*** Ejecución de programas / toma de medidas
- Los ordenadores con id~140xxx y 142xxx no comparten subred, luego no usarlos para tomar tiempos
- Al ejecutar programas sobre una sola máquina usar la versión paralela en secuencial (no un programa distinto que realice la misma tarea y tenga menos instrucciones...) ~> afecta al cálculo de la ganancia de velocidad.
- No vamos a evaluar las "otras posibles medidas"
- Ejecutar en atcgrid con =/usr/lib64/openmpi/bin/mpiexec=
- Medir tiempos con =MPI_Wtime()= en MPI o =omp_get_wtime= en OpenMP o =clock_gettime(CLOCK_REALTIME, &cgt1)= de C
*** MPI

En MPI se pueden numerar los procesos de forma bidimensional.

*** CUDA
- CUDA está pensado únicamente para floats, no enteros
- Las hebras se organizan en bloques, y dependiendo del bloque pueden tener identificaciones uni, bi, y tri-dimensionales.
** 4 y 5. Seminario de CUDA

- leer nombres de ficheros desde la línea de comandos
- abrir ficheros
- leer input0->a, input1->b y output->c
- sumar a+b->d
- comparar c, d (usar una tolerancia?)
- imprimir la suma (en la cpu, para que el compilador no se lo cargue!)


cuda: /usr/local/cuda-5.0/bin/nvcc -m64 -I/usr/local/cuda-5.0/include

Las hebras que están en el mismo bloque comparten memoria y se pueden sincronizar, las hebras de bloques distintos no.

identificación de hebras (1 dimensión):
- =blockidx.x= nivel de bloque
- =blockDim.x= número de hebras en bloque
- =threadidx.x= nivel de hebra en bloque
- =blockidx.x * blockDim.x + threadidx.x= nivel de hebra global

Cada bloque va siempre al mismo SM aunque haya más hebras en el bloque que cores en el SM.

Geforce GT 750M: "The GK107 Kepler core offers two shader blocks, called SMX, each with 192 shaders for a total of 384 shader cores that are clocked at the same speed as the processor core." [[http://www.notebookcheck.net/NVIDIA-GeForce-GT-750M.90245.0.html][src]]

Las hebras se agrupan en /warps/ de 32 hebras. Interesa que cuando sobren hebras sean en múltiplos de 32 para no desaprovechar trabajo de hebras. Además, el bloque mínimo debería ser de 32 hebras. Probar múltiples tamaños.
* Trabajos
** Titan Cray XK7 - Notas de exposición
#+AUTHOR: David Charte
#+OPTIONS: toc:nil
*** Diapositiva 1
- Titan usa la plataforma de supercomputación XK7 de Cray
*** Diapositiva 2
- Híbrido: usa GPUs y CPUs. Es el primer supercomputador híbrido en superar los 10 PFLOPs de rendimiento
- Es el sucesor de Jaguar y predecesor de Summit, que se estrenará el año que viene
*** Diapositiva 3
- 18688 nodos
- cada nodo: Opteron 6274 de 16 núcleos (32 GB RAM), NVidia Tesla K20X (6 GB RAM)
- En total casi 300k núcleos y casi 700 TB de RAM
*** Diapositiva 4
La interconexión Gemini usa topología toro-3D (la foto es simplificación)
*** Diapositiva 5
- permite un direccionamiento global de la memoria (S.O. Cray Linux Environment basado en SUSE Entreprise Server) que se puede utilizar desde lenguajes como Cray Chapel, Unified Parallel C o Co-Array Fortran
- usa conmutación vermiforme
*** Diapositiva 6
- cada 2 pares de chips Opteron se conectan al chip Gemini (SoC) usando enlaces HyperTransport3
- En las dimensiones X y Z Gemini tiene dos enlaces, en la Y uno. En total 10 links de red.
- Un chip tiene 48 puertos (8 para NICs, 10 grupos de 4 para red) implementados en un router YARC con enrutamiento adaptativo
- Ancho de banda conjunto: 168 GB/s por chip Gemini
*** Diapositiva 7
- Netlink conecta routers y NICs, se ocupa de los cambios de frecuencia de reloj
- El L0 es un procesador de control embebido que se conecta al supervisor y permite a la red Cray cargar tablas de enrutamiento, etc.
*** Diapositiva 8
- Cada NIC tiene estos módulos
- Está conectado por un lado al HT3 y por el otro al router (4 puertos cada NIC)
*** Diapositiva 9
Componentes a destacar:
- FMA: generación y almacenamiento de transferencias pequeñas (baja latencia)
- BTE: permite transferencias asíncronas entre memoria local y remota (hasta 4 GB sin intervención CPU)
- CQ: mecanismo ligero de notificación de eventos
- AMO: operaciones atómicas ($\rightarrow$ caché AMO) como Cmp&Exchg; mantiene coherencia con el host pero el host no mantiene coherencia con el NIC
- SSID: mecanismo para identificar paquetes de la misma transacción y colocarlos en orden en el destino aunque no lleguen desordenados
*** Diapositiva 10
- Cada router está organizado en tiles (los 48 puertos), para que la fabricación sea más fácil
- Cada buffer de almacenamiento se asocia con un puerto de entrada y uno de salida
*** Diapositiva 11
- Utiliza códigos correctores de errores de 16 bits para proteger hasta 64 B de datos (+ cabeceras)
* Ejercicios
** 7

Se va a diseñar un multiprocesador de memoria físicamente distribuida con 512 nodos conectados
con una red de interconexión malla tridimensional. Implementar para la red un algoritmo distribuido en
los conmutadores para encaminamiento uno-a-todos (difusión o broadcast). Considerar que el algoritmo
(que se ejecuta en el conmutador) utiliza el canal de entrada del paquete para determinar los canales de
salida por los que reenviar el paquete.

Suponemos la misma base en todas las dimensiones: 512^(1/3) = 8

Un conmutador de esta malla tiene un (?) controlador de enlace para cada dimensión
    d1 /d2/
    ||//
d0 =[]=
  //||

Observamos que no se debe distribuir el mensaje por el mismo canal por el que llegó! Para evitar duplicaciones se elige un orden de dimensiones: creciente o decreciente.

En este caso, si el mensaje llega por la d0 lo difundimos en todas dimensiones, si llega por d1 no difundimos por d0 y si llega por d2 sólo difundimos por d2.

|---------+-----+-----+-----+-----+-----+-----+---------|
| I\O     | d0+ | d0- | d1+ | d1- | d2+ | d2- | interno |
|---------+-----+-----+-----+-----+-----+-----+---------|
| d0+     | X   |     | X   | X   | X   | X   | X       |
| d0-     |     | X   | X   | X   | X   | X   | X       |
| d1+     |     |     | X   |     | X   | X   | X       |
| d1-     |     |     |     | X   | X   | X   | X       |
| d2+     |     |     |     |     | X   |     | X       |
| d2-     |     |     |     |     |     | X   | X       |
| interno | X   | X   | X   | X   | X   | X   |         |
|---------+-----+-----+-----+-----+-----+-----+---------|

Grafo de dependencias: se construyen las aristas aplicando el algoritmo de enrutamiento.
** 12
Algoritmo intervalo para mallas. La muestra la tabla de encaminamiento del
conmutador 10 para una malla 4x4 con algoritmo intervalo. Escriba la tabla de
encaminamiento para el conmutador 5 de la red 4x4 de la figura. Escribir la tabla para los
conmutadores 4, 13 y 23, en una malla 3x3x3 con algoritmo intervalo que anule la distancia
respecto al destino comenzando por la dimensión más alta, y siguiendo un orden
decreciente (D2, D1, D0).

| Canal      | Int (nodo 5) | Int (n 10 dec) | Int (n 5 dec) |
|------------+--------------+----------------+---------------|
| d0+        |         8-15 |          11-11 |           6-7 |
| d0-        |          0-3 |            8-9 |           4-4 |
| d1+        |          6-7 |          12-15 |          8-15 |
| d1-        |          4-4 |            0-7 |           0-3 |
| interno    |          5-5 |          10-10 |           5-5 |
| numeración |    creciente |    decreciente |   decreciente |

Nótese que al cambiar la ordenación (de creciente a decreciente) simplemente hay que intercambiar las entreadas de los enlaces correspondientes.

Tabla para la malla 3x3x3:

| Canal   |    4 |    13 |    23 |
|---------+------+-------+-------|
| d0+     |  5-5 |    14 |    -- |
| d0-     |  3-3 |    12 | 21-22 |
| d1+     |  6-8 | 15-17 | 24-26 |
| d1-     |  0-2 |  9-11 | 18-20 |
| d2+     | 9-26 | 18-26 |    -- |
| d2-     |   -- |   0-8 |  0-17 |
| interno |  4-4 | 13-13 | 23-23 |
